{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYM61xrTsP5d"
   },
   "source": [
    "# Retraining an Image Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BtG3ZXAFmsKe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bL54LWCHt5q5"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9598,
     "status": "ok",
     "timestamp": 1591284097487,
     "user": {
      "displayName": "Michael Locker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgiW5mgoWoxxYpGNo3m3svSMq_9vtSMeCe1uhHi1A=s64",
      "userId": "11028345317102626050"
     },
     "user_tz": 300
    },
    "id": "dlauq-4FWGZM",
    "outputId": "8e30ec98-27aa-4d0d-a98b-ee7611e1fd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.2.0\n",
      "Hub version: 0.8.0\n",
      "WARNING:tensorflow:From <ipython-input-1-0831fa394ed3>:12: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlsEcKVeuCnf"
   },
   "outputs": [],
   "source": [
    "module_selection = (\"mobilenet_v2_140_224\", 224) #@param [\"(\\\"mobilenet_v2_140_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
    "handle_base, pixels = module_selection\n",
    "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
    "\n",
    "BATCH_SIZE = 32 #@param {type:\"integer\"}\n",
    "print(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTY8qzyYv3vl"
   },
   "source": [
    "## Set up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBtFK1hO8KsO"
   },
   "outputs": [],
   "source": [
    "# data_dir = tf.keras.utils.get_file(\n",
    "#     'flower_photos',\n",
    "#     'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "#     untar=True)\n",
    "\n",
    "\n",
    "#data_dir = \"/content/drive/My Drive/myPeopleWnull\"\n",
    "data_dir = \"/content/drive/My Drive/myPeople\"\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umB5tswsfTEQ"
   },
   "outputs": [],
   "source": [
    "datagen_kwargs = dict(rescale=1./255, validation_split=.10)\n",
    "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "                   interpolation=\"bilinear\")\n",
    "\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **datagen_kwargs)\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
    "\n",
    "do_data_augmentation = True #@param {type:\"boolean\"}\n",
    "if do_data_augmentation:\n",
    "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      horizontal_flip=True,\n",
    "      width_shift_range=0.2, height_shift_range=0.2,\n",
    "      shear_range=0.2, zoom_range=0.2,\n",
    "      **datagen_kwargs)\n",
    "else:\n",
    "  train_datagen = valid_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FS_gVStowW3G"
   },
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaJW3XrPyFiF"
   },
   "outputs": [],
   "source": [
    "do_fine_tuning = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50FYNIb1dmJH"
   },
   "outputs": [],
   "source": [
    "print(\"Building model with\", MODULE_HANDLE)\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
    "    #tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(48, activation=None),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(18, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2e5WupIw2N2"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9f3yBUvkd_VJ"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_YKX2Qnfg6x"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2e5WupIw2N2"
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYOw0fTO1W4x"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,2])\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(hist[\"accuracy\"])\n",
    "plt.plot(hist[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqx6E0NzKmuU"
   },
   "outputs": [],
   "source": [
    "class_labels = sorted(valid_generator.class_indices.items(), key=lambda pair:pair[1])\n",
    "class_labels = np.array([key.title() for key, value in class_labels])\n",
    "\n",
    "for image_batch, label_batch in valid_generator:\n",
    "  print(\"Image batch shape: \", image_batch.shape)\n",
    "  print(\"Label batch shape: \", label_batch.shape)\n",
    "  break\n",
    "\n",
    "label_id = np.argmax(label_batch, axis=-1)\n",
    "predicted_class_floats = model.predict(image_batch)\n",
    "\n",
    "predicted_index = np.argmax(predicted_class_floats, axis=-1)\n",
    "predicted_label_batch = class_labels[predicted_index]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for n in range(30):\n",
    "  plt.subplot(6,5,n+1)\n",
    "  plt.imshow(image_batch[n])\n",
    "  color = \"green\" if predicted_index[n] == label_id[n] else \"red\"\n",
    "  plt.title(predicted_label_batch[n].title(), color=color)\n",
    "  \n",
    "  plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")\n",
    "\n",
    "#print(predicted_class_floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnDO1lIE13E0"
   },
   "outputs": [],
   "source": [
    "\n",
    "testFileName = 'patriciaTest1.jpg'\n",
    "directory = '/content/drive/My Drive/myTest/'\n",
    "\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "\n",
    "testImg = cv2.imread(directory + testFileName)\n",
    "ogimage = testImg.copy()\n",
    "testImg.resize(224,224,3)\n",
    "testImg = np.expand_dims(testImg, axis=0)\n",
    "print(testImg.shape)\n",
    "predicted_example = model.predict(testImg)\n",
    "print(predicted_example)\n",
    "predicted_index = np.argmax(predicted_example, axis=-1)\n",
    "print(predicted_index)\n",
    "predicted_label = class_labels[predicted_index]\n",
    "print(class_labels)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for n in range(1):\n",
    "  plt.subplot(6,5,n+1)\n",
    "  plt.imshow(ogimage) #testImg)\n",
    "  plt.title(predicted_label[n].title(), color='blue')\n",
    "  \n",
    "  plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGvTi69oIc2d"
   },
   "outputs": [],
   "source": [
    "saved_model_path = \"/content/drive/My Drive/tfServeModels/lockIndust001FactialRecognitionb3\"\n",
    "tf.saved_model.save(model, saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TFlite version of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Va1Vo92fSyV6"
   },
   "outputs": [],
   "source": [
    "optimize_lite_model = True  #@param {type:\"boolean\"}\n",
    "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
    "num_calibration_examples = 60  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "representative_dataset = None\n",
    "if optimize_lite_model and num_calibration_examples:\n",
    "  # Use a bounded number of training examples without labels for calibration.\n",
    "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
    "  representative_dataset = lambda: itertools.islice(\n",
    "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
    "      num_calibration_examples)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "if optimize_lite_model:\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  if representative_dataset:  # This is optional, see above.\n",
    "    converter.representative_dataset = representative_dataset\n",
    "lite_model_content = converter.convert()\n",
    "\n",
    "with open(\"/content/drive/My Drive/tfLiteModels/lockIndust001FactialRecognitionb3TFlite\", \"wb\") as f:\n",
    "  f.write(lite_model_content)\n",
    "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
    "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFlite for Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wqEmD0xIqeG"
   },
   "outputs": [],
   "source": [
    "# docs_infra: no_execute\n",
    "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
    "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
    "def lite_model(images):\n",
    "  interpreter.allocate_tensors()\n",
    "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
    "  interpreter.invoke()\n",
    "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute TFlite Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMMK-fZrKrk8"
   },
   "outputs": [],
   "source": [
    "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
    "# docs_infra: no_execute\n",
    "num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
    "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
    "                for batch in train_generator\n",
    "                for (image, label) in zip(*batch))\n",
    "count = 0\n",
    "count_lite_tf_agree = 0\n",
    "count_lite_correct = 0\n",
    "for image, label in eval_dataset:\n",
    "  probs_lite = lite_model(image[None, ...])[0]\n",
    "  probs_tf = model(image[None, ...]).numpy()[0]\n",
    "  y_lite = np.argmax(probs_lite)\n",
    "  y_tf = np.argmax(probs_tf)\n",
    "  y_true = np.argmax(label)\n",
    "  count +=1\n",
    "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
    "  if y_lite == y_true: count_lite_correct += 1\n",
    "  if count >= num_eval_examples: break\n",
    "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
    "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ScitaPqhKtuW"
   ],
   "name": "RESOURCEBOOK_TFHub2_Retraining_Image_Classifierb3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
